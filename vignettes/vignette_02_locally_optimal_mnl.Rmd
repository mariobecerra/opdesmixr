---
title: "Locally optimal designs for MNL models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette_02_locally_optimal_mnl}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, eval = F, include = F}
rmarkdown::render(here::here("vignettes/vignette_02_locally_optimal_mnl.Rmd"))
```


# Mixtures and MNL model

Notation:

* $q$: Number of ingredient proportions
* $S$: Number of choice sets
* $J$: Number of alternatives in each choice set


Utility of alternative $j$ in choice set $s$, denoted by $u_{js}$ as a function of the observed alternative specific attributes plus an error term:

$$
    u_{js} = \boldsymbol{f}(\boldsymbol{x}_{js})^T \boldsymbol{\beta} + \varepsilon_{js}
$$

where $\boldsymbol{x}_{js}$ is a vector that contains the $q$ proportions corresponding to alternative $j$ in choice set $s$, $\boldsymbol{f}(\boldsymbol{x}_{js})$ represents the model expansion of these attributes, and $\boldsymbol{\beta}$ is the parameter vector.

The error terms $\varepsilon_{js}$ are assumed to be i.i.d. and Gumbel distributed (a.k.a- generalized extreme value distribution type I and log-Weibull distribution).

The probability that a respondent chooses alternative $j$ in choice set $s$, denoted as $p_{js}$, is the probability that the respondent chooses the alternative that has the highest utility, defined as

$$
    p_{js} = \mathbb{P} \left[ u_{js} > max(u_{1s}, ..., u_{j-1, s}, u_{j+1, s}, ..., u_{Js} ) \right]
$$

Since the error terms are Gumbel distributed, it can be shown that

$$
    p_{js} = \frac{ \exp{ \left[ \boldsymbol{f}(\boldsymbol{x}_{js})^T \boldsymbol{\beta} \right]} }{ \sum_{t = 1}^J \exp{ \left[ \boldsymbol{f}(\boldsymbol{x}_{ts})^T \boldsymbol{\beta} \right]} }
$$

In the mixture case, using a special cubic ScheffÃ© model, we have that

$$
    \boldsymbol{f}(\boldsymbol{x}_{js})^T \boldsymbol{\beta} =
    \sum_{i = 1}^{q-1} \beta_i^* x_{ijs} + 
    \sum_{i = 1}^{q-1} \sum_{k = i + 1}^{q} \beta_{ik} x_{ijs} x_{kjs} + 
    \sum_{i = 1}^{q-2} \sum_{k = i + 1}^{q-1} \sum_{l = k + 1}^{q} \beta_{ikl} x_{ijs} x_{kjs} x_{ljs}
$$

with $\boldsymbol{x_{js}}, \boldsymbol{\beta} \in \mathbb{R}^{m-1}$ and $m = \frac{q^3 + 5q}{6}$.

Original vector of parameters $\boldsymbol{\beta_0} \in \mathbb{R}^{m-1}$ is such that

$$
    \boldsymbol{\beta} = 
    \left(
    \beta_{1}, \beta_{2}, ..., \beta_{q-1}, \beta_{1,2}, ..., \beta_{q-1,q}, ..., \beta_{q-2,q-1,q}
    \right)^T
$$

$$
    \boldsymbol{\beta_0} = 
    \left( 
    \beta_{1}^*, \beta_{2}^*, ..., \beta_{q-1}^*, \beta_{1,2}^*, ..., \beta_{q-1,q}^*, ...,  \beta_{q-2,q-1,q}^*
    \right)^T
$$

and $\beta_i^* = \beta_i - \beta_q$ for each $i \in \left\{ 1, ..., q-1 \right\}$.

That means that the first $q-1$ elements of $\boldsymbol{\beta}$ are the first $q-1$ elements of $\boldsymbol{\beta_0}$ minus the $q$-th element of $\boldsymbol{\beta_0}$.

For the MNL model, the information matrix $\boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta})$ is obtained as the sum of the information matrices of the $S$ choice sets, $\boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta})$, i.e.,

$$
    \boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta}) = \sum_{s = 1}^S \boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta})
$$

with

$$
    \boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta}) = 
        \boldsymbol{X}_s^T (\boldsymbol{P}_s - \boldsymbol{p}_s \boldsymbol{p}_s^T) \boldsymbol{X}_s
$$

i.e., 

$$
    \boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta}) = 
        \sum_{s = 1}^S 
        \boldsymbol{X}_s^T (\boldsymbol{P}_s - \boldsymbol{p}_s \boldsymbol{p}_s^T) \boldsymbol{X}_s
$$

with 

* $\boldsymbol{X}_s^T = \left[ \boldsymbol{f}(\boldsymbol{x}_{js}) \right]_{j \in \left\{ 1, ..., J \right\}}$ and $\boldsymbol{X}_s \in \mathbb{R}^{J \times (m-1)}$
* $\boldsymbol{p}_s^T = \left( p_{1s}, ..., p_{Js} \right)$
* $\boldsymbol{P}_s = \mathrm{diag}(\boldsymbol{p}_s) \in \mathbb{R}^{J \times J}$

with $\boldsymbol{X}$ denoting the design matrix.

The information matrices $\boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta})$ and $\boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta})$ are of dimension $(m-1) \times (m-1)$.


# Classical optimal design

### D-optimality

D-optimality focuses on the determinant of the information matrix. It's defined as 


$$
\mathcal{D} = \log{\left( 
    \det \left( 
         \left[ 
            \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta})
         \right) \right]^{\frac{1}{m-1}} 
    \right)}
$$

### I-optimality


I-optimality or V-optimality deals with average prediction variance in the design space $\chi$. The I-optimality criterion is

$$
\mathcal{I} =
    \int_{\chi} \boldsymbol{c}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{c}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}
$$

with

$$
    \boldsymbol{c}(\boldsymbol{x}_{js}) =
    \frac{\partial p_{js}}{\partial \boldsymbol{\beta}} =
    p_{js} \left( \boldsymbol{x}_{js} - \sum_{t = 1}^J p_{ts} \boldsymbol{x}_{ts} \right)
$$

Using the cyclic property of the trace, it can be shown that

$$
    \int_{\chi} \boldsymbol{c}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{c}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js} 
    =
    \mathrm{tr}\left[ \boldsymbol{W} \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \right]
$$

where $\boldsymbol{W}$ is called the moments matrix and is such that $\boldsymbol{W} = \int_{\chi} \boldsymbol{c}(\boldsymbol{x}_{js}) \boldsymbol{c}^T(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}$.

In the mixture case, $\chi$ is the $(q-1)$-dimensional simplex. Unfortunately, there is no closed formula to solve this integral, so the integral needs to be numerically approximated for each $\boldsymbol{\beta}$ and $\boldsymbol{x}_{js}$, which is very computationally expensive.

A more attractive alternative is to focus on the variance of the predicted utility instead of the variance of the predictive probability

$$
    \mathrm{Var} \left[ \hat{u_{js}} \right] =
        \boldsymbol{f}^T(\boldsymbol{x}_{js}) \mathrm{Var} \left[ \boldsymbol{\hat{\beta}} \right] \boldsymbol{f}(\boldsymbol{x}_{js}) =
        \boldsymbol{f}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{f}(\boldsymbol{x}_{js})
$$

Then the average variance of the predicted utility is

$$
    \int_{\chi} \boldsymbol{f}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{f}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}
    =
    \int_{\chi} \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{f}(\boldsymbol{x}_{js}) \boldsymbol{f}^T(\boldsymbol{x}_{js}) \right] d\boldsymbol{x}_{js} 
    =
    \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{W} \right]
$$

with $\boldsymbol{W}$ the newly defined moments matrix of the form $\boldsymbol{W} = \int_{\chi}  \boldsymbol{f}(\boldsymbol{x}_{js}) \boldsymbol{f}^T(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js} $

Since we are considering the case in which $\boldsymbol{x}_{js}$ are mixtures and, hence, $\chi$ is the $(q-1)$-dimensional simplex, then the moments matrix $\boldsymbol{W}$ has a closed form which is not shown here.

Then, using the predicted utility instead of the predicted probability we get the following I-optimality criterion

$$
\mathcal{I} = \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{W} \right]
$$


# Examples on how to create locally optimal designs

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = T, 
  fig.width = 8
)
```


```{r setup}
library(opdesmixr)
library(tidyverse)

n_cores = parallel::detectCores()
```

### Utility neutral designs

```{r}
q = 3
J = 3
S = 8
X2 = mnl_create_random_initial_design(q, J, S, seed = 4)
beta2 = rep(0, (q*q*q + 5*q)/6)

opt2_des_beta2_D = mnl_mixture_coord_exch(
  q = q, J = J, S = S, 
  n_random_starts = 50,
  beta = beta2,
  opt_crit = "D",
  verbose = 0,
  plot_designs = T,
  max_it = 20,
  seed = 10,
  n_cores = n_cores
)


opt2_des_beta2_I = mnl_mixture_coord_exch(
  q = q, J = J, S = S, 
  n_random_starts = 50,
  beta = beta2,
  opt_crit = "I",
  verbose = 0,
  plot_designs = T,
  max_it = 20,
  seed = 10,
  n_cores = n_cores
)

```

Supplying a random initial design:

```{r}
X2 = mnl_create_random_initial_design(q, J, S, seed = 4)

X2_opt_beta2_D = mnl_mixture_coord_exch(
  X = X2,
  beta = beta2,
  opt_crit = "D",
  verbose = 0,
  plot_designs = T,
  max_it = 100
)

X2_opt_beta2_I = mnl_mixture_coord_exch(
  X = X2,
  beta = beta2,
  opt_crit = "I",
  verbose = 0,
  plot_designs = T,
  max_it = 100
)
```


### Supplying beta vector

With only one initial random design:

```{r}

q = 3
J = 3
S = 6
X3 = mnl_create_random_initial_design(q, J, S, seed = 3)
beta3_2 = create_random_beta(q)

X3_2_opt_D = mnl_mixture_coord_exch(
  X = X3,
  beta = beta3_2$beta,
  opt_crit = "D",
  verbose = 0,
  plot_designs = T,
  max_it = 50,
)

X3_2_opt_I = mnl_mixture_coord_exch(
  X = X3,
  beta = beta3_2$beta,
  opt_crit = "I",
  verbose = 0,
  plot_designs = T,
  max_it = 50,
)
```

With 50 initial random designs:

```{r}
opt2_des_beta3_D = mnl_mixture_coord_exch(
  q = q, J = J, S = S, 
  n_random_starts = 50,
  beta = beta3_2$beta,
  opt_crit = "D",
  verbose = 0,
  plot_designs = T,
  max_it = 20,
  seed = 10,
  n_cores = n_cores
)

opt2_des_beta3_I = mnl_mixture_coord_exch(
  q = q, J = J, S = S, 
  n_random_starts = 50,
  beta = beta3_2$beta,
  opt_crit = "I",
  verbose = 0,
  plot_designs = T,
  max_it = 20,
  seed = 10,
  n_cores = n_cores
)
```

