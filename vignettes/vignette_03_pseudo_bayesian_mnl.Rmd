---
title: "Pseudo-Bayesian efficient designs for MNL models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette_03_pseudo_bayesian_mnl}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---


```{r, eval = F, include = F}
rmarkdown::render(here::here("vignettes/vignette_03_pseudo_bayesian_mnl.Rmd"))
```


# Mixtures and MNL model

Notation:

* $q$: Number of ingredient proportions
* $S$: Number of choice sets
* $J$: Number of alternatives in each choice set

Utility of alternative $j$ in choice set $s$, denoted by $u_{js}$ as a function of the observed alternative specific attributes plus an error term:

$$
    u_{js} = \boldsymbol{f}(\boldsymbol{x}_{js})^T \boldsymbol{\beta} + \varepsilon_{js}
$$

where $\boldsymbol{x}_{js}$ is a vector that contains the $q$ proportions corresponding to alternative $j$ in choice set $s$, $\boldsymbol{f}(\boldsymbol{x}_{js})$ represents the model expansion of these attributes, and $\boldsymbol{\beta}$ is the parameter vector.

The error terms $\varepsilon_{js}$ are assumed to be i.i.d. and Gumbel distributed (a.k.a- generalized extreme value distribution type I and log-Weibull distribution).

The probability that a respondent chooses alternative $j$ in choice set $s$, denoted as $p_{js}$, is the probability that the respondent chooses the alternative that has the highest utility, defined as

$$
    p_{js} = \mathbb{P} \left[ u_{js} > max(u_{1s}, ..., u_{j-1, s}, u_{j+1, s}, ..., u_{Js} ) \right]
$$

Since the error terms are Gumbel distributed, it can be shown that

$$
    p_{js} = \frac{ \exp{ \left[ \boldsymbol{f}(\boldsymbol{x}_{js})^T \boldsymbol{\beta} \right]} }{ \sum_{t = 1}^J \exp{ \left[ \boldsymbol{f}(\boldsymbol{x}_{ts})^T \boldsymbol{\beta} \right]} }
$$

In the mixture case, using a special cubic Scheff√© model, we have that

$$
    \boldsymbol{f}(\boldsymbol{x}_{js})^T \boldsymbol{\beta} =
    \sum_{i = 1}^{q-1} \beta_i^* x_{ijs} + 
    \sum_{i = 1}^{q-1} \sum_{k = i + 1}^{q} \beta_{ik} x_{ijs} x_{kjs} + 
    \sum_{i = 1}^{q-2} \sum_{k = i + 1}^{q-1} \sum_{l = k + 1}^{q} \beta_{ikl} x_{ijs} x_{kjs} x_{ljs}
$$

with $\boldsymbol{x_{js}}, \boldsymbol{\beta} \in \mathbb{R}^{m-1}$ and $m = \frac{q^3 + 5q}{6}$.

Original vector of parameters $\boldsymbol{\beta_0} \in \mathbb{R}^{m-1}$ is such that

$$
    \boldsymbol{\beta} = 
    \left(
    \beta_{1}, \beta_{2}, ..., \beta_{q-1}, \beta_{1,2}, ..., \beta_{q-1,q}, ..., \beta_{q-2,q-1,q}
    \right)^T
$$

$$
    \boldsymbol{\beta_0} = 
    \left( 
    \beta_{1}^*, \beta_{2}^*, ..., \beta_{q-1}^*, \beta_{1,2}^*, ..., \beta_{q-1,q}^*, ...,  \beta_{q-2,q-1,q}^*
    \right)^T
$$

and $\beta_i^* = \beta_i - \beta_q$ for each $i \in \left\{ 1, ..., q-1 \right\}$.

That means that the first $q-1$ elements of $\boldsymbol{\beta}$ are the first $q-1$ elements of $\boldsymbol{\beta_0}$ minus the $q$-th element of $\boldsymbol{\beta_0}$.

For the MNL model, the information matrix $\boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta})$ is obtained as the sum of the information matrices of the $S$ choice sets, $\boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta})$, i.e.,

$$
    \boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta}) = \sum_{s = 1}^S \boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta})
$$

with

$$
    \boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta}) = 
        \boldsymbol{X}_s^T (\boldsymbol{P}_s - \boldsymbol{p}_s \boldsymbol{p}_s^T) \boldsymbol{X}_s
$$

i.e., 

$$
    \boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta}) = 
        \sum_{s = 1}^S 
        \boldsymbol{X}_s^T (\boldsymbol{P}_s - \boldsymbol{p}_s \boldsymbol{p}_s^T) \boldsymbol{X}_s
$$

with 


* $\boldsymbol{X}_s^T = \left[ \boldsymbol{f}(\boldsymbol{x}_{js}) \right]_{j \in \left\{ 1, ..., J \right\}}$ and $\boldsymbol{X}_s \in \mathbb{R}^{J \times (m-1)}$
* $\boldsymbol{p}_s^T = \left( p_{1s}, ..., p_{Js} \right)$
* $\boldsymbol{P}_s = \mathrm{diag}(\boldsymbol{p}_s) \in \mathbb{R}^{J \times J}$

with $\boldsymbol{X}$ denoting the design matrix.

The information matrices $\boldsymbol{I}(\boldsymbol{X}, \boldsymbol{\beta})$ and $\boldsymbol{I}_s(\boldsymbol{X}, \boldsymbol{\beta})$ are of dimension $(m-1) \times (m-1)$.








## Classical and Bayesian optimal design

#### D-optimality

Pseudo-Bayesian designs average the prior distribution.

Local D-optimality criterion: 

$$
    \mathcal{D} =
\log{\left( 
    \det \left( 
         \left[ 
            \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta})
         \right) \right]^{\frac{1}{m-1}} 
    \right)}
$$
 
Pseudo-Bayesian D-optimality criterion: 

$$
    \mathcal{D}_B =
\log{\left( 
    \int_{\mathbb{R}^{m-1}}
    \left[  
        \det \left( 
            \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta})
         \right) \right]^{\frac{1}{m-1}} 
    \pi(\boldsymbol{\beta}) d\boldsymbol{\beta}
    \right)}
$$


where $\pi(\boldsymbol{\beta})$ is the prior distribution of $\boldsymbol{\beta}$.

#### I-optimality

I-optimality or V-optimality deals with average prediction variance in the design space $\chi$.

Local I-optimality criterion:

$$
    \mathcal{I} =
    \int_{\chi} \boldsymbol{c}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{c}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}
$$

Pseudo-Bayesian I-optimality criterion: 

$$
    \mathcal{I}_B =
    \int_{\mathbb{R}^{m-1}}
    \int_{\chi} \boldsymbol{c}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{c}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}
    \pi(\boldsymbol{\beta}) d\boldsymbol{\beta}
$$

with

$$
    \boldsymbol{c}(\boldsymbol{x}_{js}) =
    \frac{\partial p_{js}}{\partial \boldsymbol{\beta}} =
    p_{js} \left( \boldsymbol{x}_{js} - \sum_{t = 1}^J p_{ts} \boldsymbol{x}_{ts} \right).
$$
    

Using the cyclic property of the trace, it can be shown that

$$
    \int_{\chi} \boldsymbol{c}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{c}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js} 
    =
    \mathrm{tr}\left[ \boldsymbol{W} \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \right]
$$

where $\boldsymbol{W}$ is called the moments matrix and is such that $\boldsymbol{W} = \int_{\chi} \boldsymbol{c}(\boldsymbol{x}_{js}) \boldsymbol{c}^T(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}$.

In the mixture case, $\chi$ is the $(q-1)$-dimensional simplex. Unfortunately, there is no closed formula to solve this integral, so the integral needs to be numerically approximated for each $\boldsymbol{\beta}$ and $\boldsymbol{x}_{js}$, which is very computationally expensive.

A more attractive alternative is to focus on the variance of the predicted utility instead of the variance of the predictive probability

$$
    \mathrm{Var} \left[ \hat{u_{js}} \right] =
        \boldsymbol{f}^T(\boldsymbol{x}_{js}) \mathrm{Var} \left[ \boldsymbol{\hat{\beta}} \right] \boldsymbol{f}(\boldsymbol{x}_{js}) =
        \boldsymbol{f}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{f}(\boldsymbol{x}_{js})
$$

Then the average variance of the predicted utility is

$$
    \int_{\chi} \boldsymbol{f}^T(\boldsymbol{x}_{js}) \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{f}(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js}
    =
    \int_{\chi} \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{f}(\boldsymbol{x}_{js}) \boldsymbol{f}^T(\boldsymbol{x}_{js}) \right] d\boldsymbol{x}_{js} 
    =
    \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{W} \right]
$$

with $\boldsymbol{W}$ the newly defined moments matrix of the form $\boldsymbol{W} = \int_{\chi}  \boldsymbol{f}(\boldsymbol{x}_{js}) \boldsymbol{f}^T(\boldsymbol{x}_{js}) d\boldsymbol{x}_{js} $

Since we are considering the case in which $\boldsymbol{x}_{js}$ are mixtures and, hence, $\chi$ is the $(q-1)$-dimensional simplex, then the moments matrix $\boldsymbol{W}$ has a closed form (which is not shown here).


Then, using the predicted utility instead of the predicted probability we get the following criteria:

Local I-optimality criterion:
$$
    \mathcal{I} = \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{W} \right]
$$

Pseudo-Bayesian I-optimality criterion: 
$$
    \mathcal{I}_B =\int_{\mathbb{R}^{m-1}}
    \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}) \boldsymbol{W} \right]
    \pi(\boldsymbol{\beta}) d\boldsymbol{\beta}
$$


Focusing on the predicted utility instead of the predicted probability makes sense and also provides a much easier way to compute designs.

Advantages:


* $\boldsymbol{W}$ does not depend on $\boldsymbol{\beta}$, so it only needs to be computed once, making the whole process less computationally intensive
    
* Predicted probabilities depend on the composition of the choice set, whereas the mixture is absolute and depends only on the utility
    
* The moments matrix of the predicted probability needs to be numerically approximated if the design space is continuous (as in this particular case)
    



In the pseudo-Bayesian case, we can use a Monte-Carlo approximation to the integrals involving the priors

$$
    \mathcal{D}_B \approx 
        \log{\left( 
        \frac{1}{R} \sum_{i = 1}^R
        \left[  
        \det \left( 
            \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}^{(i})
         \right) \right]^{\frac{1}{m-1}} 
        \right)}
$$
    
    
$$
    \mathcal{I}_B \approx
        \frac{1}{R} \sum_{i = 1}^R
        \mathrm{tr} \left[ \boldsymbol{I}^{-1}(\boldsymbol{X}, \boldsymbol{\beta}^{(i)}) \boldsymbol{W} \right]
$$
    


where $\left\{ \boldsymbol{\beta}^{(i} \right\}_{i = 1}^R$ are independent random draws from the prior distribution $\pi(\boldsymbol{\beta})$.

To get the $R$ draws from the prior, Halton sequences are often used since they are known to provide better approximations to integrals compared to other methods. This is because Halton sequences provide good coverage of the entire density domain, as well as negatively correlated draws that reduce the variance of the approximation to the integral.


# Examples on how to create pseudo-Bayesian efficient designs for MNL models


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = T, 
  fig.width = 8
)
```


```{r setup}
library(opdesmixr)
library(tidyr)
library(dplyr)
library(ggplot2)

theme_set(theme_bw())
n_cores = parallel::detectCores()
```

### Special cubic Scheff√© model with 3 ingredients

Plot prior distributions of a random beta parameter. We choose 120 Halton draws from a Gaussian distribution.

```{r}


q = 3
J = 5
S = 4

beta_q3_o3 = create_random_beta(q, order = 3, seed = 3)
beta_q3_o3_prior_draws = get_halton_draws(beta_q3_o3$beta)


beta_q3_o3_prior_draws %>%
  as.data.frame() %>%
  pivot_longer(starts_with("V")) %>%
  ggplot() +
  geom_density(aes(value)) +
  facet_wrap(~name)




```

#### D-efficient designs using the discretization of Cox direction and Brent's method as optimization methods

Discretization:

```{r}
# 25 secs
# 56 secs
X_q3_J5_s4_D_bayes_o3 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o3_prior_draws,
  n_cox_points = 20,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "D",
  opt_method = "D"
)


```

Brent's method:

```{r}
# 16 seconds
# 40 secs
X_q3_J5_s4_D_bayes_o3 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o3_prior_draws,
  n_cox_points = 20,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "D",
  opt_method = "B"
)
```


#### I-efficient designs using the discretization of Cox direction and Brent's method as optimization methods

Discretization:

```{r}
# 100 secs
X_q3_J5_s4_I_bayes_o3 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o3_prior_draws,
  n_cox_points = 20,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "I",
  opt_method = "D"
)

```

Brent's method:

```{r}

# 70 secs
X_q3_J5_s4_I_bayes_o3 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o3_prior_draws,
  n_cox_points = 20,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "I",
  opt_method = "B"
)
```



### Second order Scheff√© model with 3 ingredients

Plot prior distributions of a random beta parameter. We choose 120 Halton draws from a Gaussian distribution.

```{r}


q = 3
J = 5
S = 4

beta_q3_o2 = create_random_beta(q, order = 2, seed = 2)
beta_q3_o2_prior_draws = get_halton_draws(beta_q3_o2$beta)



beta_q3_o2_prior_draws %>%
  as.data.frame() %>%
  pivot_longer(starts_with("V")) %>%
  ggplot() +
  geom_density(aes(value)) +
  facet_wrap(~name)




```

#### D-efficient design using Brent's method as optimization method

```{r}
# # 20 secs
# (t1 = Sys.time())
X_q3_J5_s4_D_bayes_o2 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o2_prior_draws,
  order = 2,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "D",
  opt_method = "B"
)
# (t2 = Sys.time())
# t2 - t1

```

#### I-efficient designs using Brent's method as optimization method

```{r}

# # 26 secs
# (t1 = Sys.time())
X_q3_J5_s4_I_bayes_o2 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o2_prior_draws,
  order = 2,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "I",
  opt_method = "B"
)
# (t2 = Sys.time())
# t2 - t1

```







### First order Scheff√© model with 3 ingredients

Plot prior distributions of a random beta parameter. We choose 120 Halton draws from a Gaussian distribution.

```{r}


q = 3
J = 5
S = 4

beta_q3_o1 = create_random_beta(q, order = 1, seed = 2)
beta_q3_o1_prior_draws = get_halton_draws(beta_q3_o1$beta)



beta_q3_o1_prior_draws %>%
  as.data.frame() %>%
  pivot_longer(starts_with("V")) %>%
  ggplot() +
  geom_density(aes(value)) +
  facet_wrap(~name)




```

#### D-efficient design using Brent's method as optimization method

```{r}
# # 6 secs
# (t1 = Sys.time())
X_q3_J5_s4_D_bayes_o1 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o1_prior_draws,
  order = 1,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "D",
  opt_method = "B"
)
# (t2 = Sys.time())
# t2 - t1

```

#### I-efficient designs using Brent's method as optimization method

```{r}

# # 18 secs
# (t1 = Sys.time())
X_q3_J5_s4_I_bayes_o1 = mnl_mixture_coord_exch(
  n_random_starts = 8,
  q = q,
  J = J,
  S = S,
  beta = beta_q3_o1_prior_draws,
  order = 1,
  max_it = 4,
  verbose = 1,
  plot_designs = T,
  n_cores = 4,
  seed = 10,
  opt_crit = "I",
  opt_method = "B"
)
# (t2 = Sys.time())
# t2 - t1

```


### Special cubic Scheff√© model with 4 ingredients



```{r}

q = 4
J = 3
S = 10

beta_q4_01 = create_random_beta(q)
beta_q4_01_prior_draws = get_halton_draws(beta_q4_01$beta)


beta_q4_01_prior_draws %>%
  as.data.frame() %>%
  pivot_longer(starts_with("V")) %>%
  ggplot() +
  geom_density(aes(value)) +
  facet_wrap(~name)



# 42 secs
(t1 = Sys.time())
X_q4_J2_s10_D_bayes_o3 = mnl_mixture_coord_exch(
  n_random_starts = 4,
  q = q,
  J = J,
  S = S,
  beta = beta_q4_01_prior_draws,
  order = 3,
  max_it = 4,
  verbose = 1,
  plot_designs = T, # if this argument is set to T, it will plot the final design
  n_cores = 4,
  seed = 10,
  opt_crit = "D",
  opt_method = "B"
)
(t2 = Sys.time())
t2 - t1






# 55 secs
(t1 = Sys.time())
X_q4_J2_s10_I_bayes_o3 = mnl_mixture_coord_exch(
  n_random_starts = 4,
  q = q,
  J = J,
  S = S,
  beta = beta_q4_01_prior_draws,
  order = 3,
  max_it = 4,
  verbose = 1,
  plot_designs = T,  # if this argument is set to T, it will plot the final design
  n_cores = 4,
  seed = 10,
  opt_crit = "I",
  opt_method = "B"
)
(t2 = Sys.time())
t2 - t1






```

The designs can also be plotted this way:

```{r}
mnl_plot_result(X_q4_J2_s10_D_bayes_o3, "original", cex_points = 0.5)
mnl_plot_result(X_q4_J2_s10_D_bayes_o3, "final", cex_points = 0.5)

mnl_plot_result(X_q4_J2_s10_I_bayes_o3, "original", cex_points = 0.5)
mnl_plot_result(X_q4_J2_s10_I_bayes_o3, "final", cex_points = 0.5)
```

